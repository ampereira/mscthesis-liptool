
\chapter{State of the Art}

Most of today’s programmers produce code and design applications using sequential programming paradigms. The application behavior is designed and tested only for sequential execution, where the only parallelism is made by the compiler at the instruction level. A few years ago a transition from single core very fast CPUs to slightly slower multicore CPUs started to happen. Unfortunately, these newer CPUs need a different programming paradigm to get the most performance possible when designing an application; however, programmers did not accompany this transition.

Programming for multicore environments require some knowledge of the underlying architectural concepts. Shared memory, cache coherence and consistency and data races are architectural aspects that the programmer did not have to face in sequential programming paradigms. Now, when designing an application, all these aspects must be taken into account, not only to ensure efficient use of the computational resources, but also the correctness of the application.

Heterogeneous computer architectures are becoming increasingly popular. They combine the flexibility of multicore CPUs with the specific capabilities of many-core accelerator devices, connected by PCI-Express interfaces. However, most computational algorithms and applications are designed with the specific characteristics of CPUs in mind. Even multithreaded applications cannot be easily ported to these devices and expect high performance. To optimize the code for these specific devices it is necessary to deeply understand the architectural principles behind their design.

These devices are usually made from small processing units, focused on achieving the most performance possible on specific problem domains, opposed to common all-around CPUs. Usually, they are oriented for massive data parallelism processing (SIMD architectures), offloading the CPU from such data intensive operations. Several many-core accelerator devices are available, ranging from the general purpose GPUs, the \intel Many Integrated Core line, currently known as \intel Xeon Phi \cite{Intel:MIC}, and Digital Signal Processors \cite{Texas:DSP}. A heterogeneous system may have one or more accelerator devices of the same or different types.

Many libraries and frameworks were already developed with these new heterogeneous platforms in mind. They range from frameworks to abstract the inherent complexity of these systems, such as OpenACC \cite{OpenACC} or GAMA \cite{GAMA}, to specialized high performance libraries for some specific scientific domains, such as CuBLAS \cite{NVIDIA:CuBLAS}.

A more in-depth analysis of these two groups of state of the art technology (hardware and software) will be presented through the next sections.

\section{Hardware}

While having the same (conceptual) purpose, different accelerator devices opt to use different approaches to solve their domain specific problems, leading to small, but important, architectural differences. If these details are not taken into account, it is impossible to make efficient code, underusing the specialized resources of these devices.

The Single Instruction Multiple Data (SIMD) parallelism model is common ground for most accelerator devices architectures. It is designed to get the most throughput when processing information by applying the same instruction, in parallel, to large sets of independent data. Considering the GPUs as an example, each pixel that must be rendered is independent from all other pixels, but the same instructions are executed, thus making their processing embarrassingly parallel. For achieving maximum performance, one important characteristic of the code is that it needs to take advantage of the most parallelism possible between the data to be processed. Other device specific properties, with interest for the programmer, will be discussed later.

Load balancing is always a challenge when programming for parallel environments. Even when using only multicore CPUs, it is important to manage how much load each core is working on so that every core is processing most of the time. If the workload is badly distributed, there will be cores stalled waiting for others to complete, wasting the available computatuinal resources. However, it also depends on the nature of the problem; regular problems are easier to balance than irregular problems, which usually require a dynamic load balancing strategy at runtime since the execution time of the parallel tasks is not predictable.

These heterogeneous architectures open the possibility of running parallel tasks on both CPU and accelerators simultaneously. However, due to their technical differences, the same task can take different amount of time to complete, depending in where it is executed. This creates another layer of complexity when dealing with the work balance. Now, while managing the work distribution inside the CPU (between its cores), and also inside the accelerator device chip, it is also important to manage the distribution between CPU and accelerator device. It is important to have a good control over the load balancing, specially in these hybrid systems, in such a way that neither of the processing units (CPU and accelerator devices) becomes stalled waiting for the other to complete, and thus not wasting any computational resources.

\subsection{Graphics Processing Unit}

There are several accelerator devices currently arriving, or already, on the market. The first, and most common, are General Purpose Graphics Processing Unit (GPGPU). Recently, GPGPU makers allowed drivers to execute code that is not related to rendering. However, there are specific hardware details that were designed only for image rendering purposes, which limit the utilization of these devices for certain types of algorithms. One example was the use of only single precision float point arithmetic in the early GPGPUs design.

As mentioned before, this type of devices are specialized for massive data parallelism, where the same instruction is applied to large amounts of data simultaneously. One example of a problem domain that can take advantage of these characteristics is the multiplication of matrices, which are very common in scientific applications. As GPGPUs evolved, the support for specific scientific demands, other than image rendering, was added, such as support for double precision float point arithmetic and compliance to all IEEE arithmetic rules.

More recently, \nvidia \cite{NVIDIA} launched a line of GPUs designed for scientific computation rather than image processing \cite{NVIDIA:Tesla}. This category of devices, known as the Tesla, has more GDDR ram, processing units and a slight different design suitable for use in cluster computational nodes (in terms of size and cooling). In this dissertation two different \nvidia GPUs will be used, the \nvidia Tesla C2070 (Fermi architecture \cite{NVIDIA:Fermi}) and the new \nvidia Tesla \textbf{GK110} based (Kepler architecture \cite{NVIDIA:Kepler}).

\nvidia GPUs architecture has two main components: Streaming Multiprocessors (SM) and GDDR5 ram. Each SM contains a set of \cuda cores, which are processing units that perform both integer and float point arithmetic (additions, multiplications and divisions). These SMs also have some specialized processing units for only square roots, sins and cosines, as well as a warp scheduler (warps will later be explained), which match \cuda threads to \cuda cores, load and store units, register files and a 2 level cache.

\nvidia considers that a parallel task is represented by a set of \cuda threads, which will execute the same instructions (however, conditional jumps are a special case that will be explained next) but on different data. A simple way to visualize this concept is by considering the problem of multiplying a scalar with a matrix as an example. In this case, a single thread will handle the multiplication of the scalar by an element of the matrix, and it is needed to use as many \cuda threads as matrix elements.

A block is a set of \cuda threads that is matched by the global scheduler to run on a specific SM. A grid is a set of blocks, representing the whole parallel task. Considering the scalar-matrix multiplication example, each \cuda thread calculates the value of an element of the matrix, and they are organized in blocks, which represent all the calculations of a single line of the matrix. The grid holds all the blocks responsible for calculating all the new values of the matrix. Note that both the block and the grid have a limited size.

A warp is a set of \cuda threads (usually the same as the number of the \cuda cores available in a SM), scheduled by the SM scheduler to run on its SM at a given time.

When programming for these devices, conditional jumps must be avoided at all costs. Within an SM it is not possible to have 2 threads executing different instructions at the same time. So, if there is a divergence between the threads within the same warp, the two conditional branches will be executed sequentially, doubling the warp execution time.

Since the GPU is connected by PCI-Express interface, the bandwidth is restricted to only 12 GB/s (6 GB in each direction of the communication). Memory transfers between the CPU and GPU must be minimal as it greatly restricts the performance.
Architecture specific details, relevant to the programmer, of both Fermi and Kepler will be presented next.

\subsubsection{\nvidia Fermi Architecture}

The relevant architectural details of this architecture, specifically for the Tesla C2070, are explained in this section. The Fermi architecture is schematized in figure \ref{fig:fermi}.

\begin{figure}[!htp]
	\begin{center}
		\includegraphics[scale=0.25]{../../common/img/fermi_arch.png}
		\caption{Schematic representation of the \nvidia Fermi architecure.}
		\label{fig:fermi}
	\end{center}
\end{figure}

In the Tesla C2070, each SM has 32 \cuda cores, with 14 SM per chip, making a total of 448 \cuda cores. Theoretically, it is possible to have 448 \cuda threads running at the same time. In each SM there is 4 Special Functional Units (SFU) to process square roots, sins and cosines.

Memory wise, these devices have a slightly different memory hierarchy than the CPUs, but still with the faster and smaller memory is closer to the \cuda cores. Each \cuda thread can have up to 63 registers, but when large amounts of threads are used this amount diminishes, which can, in some cases, lead to register spilling (when there is not enough registers to hold the variables values and they must be stored in the cache).

Within a SM there is a block of configurable 64 KB memory. In this architecture it is possible to use it as 16 KB for L1 cache and 48 KB for shared memory (only shared between threads of the same block) or vice versa. The best configuration is dependent of the specific characteristics of each algorithm, and usually requires some preliminary tests to evaluate which configuration obtains the best performance. Shared memory can also be used to hold common resources to the threads, even if they are read only, avoiding accesses to the slower global memory.

The L2 cache is slower but larger, with the size of 768 KB. It is shared among the SMs, opposed to the L1 cache. The global memory is the last level of on device memory. The Tesla C2070 has a total of 6 GB GDDR5 ram, with a bandwidth of 192.4 GB/s.

One important detail for efficient memory usage is to perform coalesced memory accesses. Since the load units get memory in blocks of 128 bits, it is possible to reduce the amount of loads by guaranteeing that all the threads that need to load data, preferably if it is continuous on the address space (such as elements of an array), do it at the same time. This allows the memory controller to find the best grouping of thread loads and consolidates them in the fewer memory accesses possible \cite{NVIDIA:Fermi}.

Finally, on the Fermi architecture it is only possible to run one kernel (piece of \cuda code designed to be ran by each \cuda thread) at a time on the GPU.

\subsubsection{\nvidia Kepler Architecture}

The Kepler and Fermi architectures have many similarities so only the relevant differentiating aspects will be presented.

The Streaming Multiprocessor present in the Fermi architecture was changed to hold more, but smaller, \cuda cores (now 192), working at half the speed of the previous \cuda cores, and it is now known as SMX. This allows having up to 2880 \cuda cores in only one chip, which can be seen in the representation of the Kepler architecture in figure \ref{fig:kepler}.

The maximum amount of registers per \cuda thread was increased from 63 to 255. A new read-only cache of 48 KB was added at the same hierarchy level of the L1 cache. The size of the L1/shared memory block is the same as Fermi, but adds a new configuration of 32/32 KB for each type. The L2 cache size has increased to 1536 KB, and its hit bandwidth is 73\% larger than on Fermi.

\begin{figure}[!htp]
	\begin{center}
		\includegraphics[scale=0.4]{../../common/img/kepler_arch.jpg}
		\caption{Schematic representation of the \nvidia Kepler architecture.}
		\label{fig:kepler}
	\end{center}
\end{figure}

Programming-wise, a set of new important features has been added to this architecture. One of them is the Dynamic Parallelism. Now it is possible to \cuda threads spawn other threads, without it being explicitly required by the host (CPU). It allows for improvements in irregular problems, such as Monte Carlo ray tracing. Another feature is the Hyper-Q, which allows multiple cores of the same CPU to use and spawn kernels on the same GPU. Also, it is now possible to run several different kernels in the same GPU at the same time, where they will be scheduled to different SMX. Finally, a new shuffle instruction has been added to the instruction set. By using this instruction \cuda threads can now read values directly from each other, within the same warp, without the need of using shared memory \cite{NVIDIA:Kepler}.

\subsection{\intel Many Integrated Core}

The \intel Many Integrated Core (MIC), currently known as \intel Xeon Phi, architecture from Intel, Knights Corner, has a different conceptual design than the Nvidia GPUs. A chip can have up to 61 multithread cores, with 4 threads each, and focus more on vector instructions \cite{Intel:MIC:Elgar}. 

\begin{figure}[!htp]
	\begin{center}
		\includegraphics[scale=0.5]{../../common/img/mic_arch.png}
		\caption{Schematic representation of the \intel MIC architecture.}
		\label{fig:mic}
	\end{center}
\end{figure}

It has 32 512 bit wide vector registers per core, with the capacity of holding 16 single precision float point values. The L2 cache size is 512 KB per core and the chip comes with 6 to 8 GB of GDDR5 ram, providing up to 320 GB/s of throughput. It was designed for memory bound problems, but Intel will also launch a different version of the chip tuned for compute bound problems.

Unlike the CPUs, the MIC cores do not share any cache, therefore cache consistency and coherence is not assured. If needed, data must be explicitly passed between cores, as in a distributed memory system. The cores are connected in a ring network, as represented in figure \ref{fig:mic}.

The MIC uses the same instruction set as common \intel CPUs (x86). This allows to easily port current libraries to run on this device. Furthermore, Intel has already announced that a tuned MPI library will be available for this device.

\section{Software}

Application development for homogeneous systems with multicore CPUs is not as recent as one may think. There are some libraries that attempt to abstract the programmer from specific architectural and implementation details, providing an easy API as close as possible to current sequential programming paradigms.

However, developing applications for heterogeneous systems, with both CPU and accelerator devices, poses a series of new challenges due to its different programming paradigm. Even though, there are some frameworks that attempt to abstract the inherent complexity of these systems.

Frameworks that attempt to ease the programmer’s job, while providing scalable and flexible solutions, which will be used during the dissertation, will be presented through the next subsections. Other frameworks that will not be used, such as Threading Building Blocks from \intel \cite{Intel:TBB}, will not be discussed.

\subsection{OpenMP}

For shared memory systems, where there is one or more multicore CPUs sharing the same memory address space, one of the most popular libraries is OpenMP \cite{OpenMP}. This API is designed for multi-platform shared memory parallel programming in C, C++ and Fortran, on all available CPU architectures. It is portable and scalable, aiming to provide a simple and flexible interface for developing parallel applications, even for the most inexperienced programmers.

While being simple to use, OpenMP allows fine-tuning of the code for the most experienced programmers, providing various task schedulers, as well as instructions for controlling more efficiently the shared memory accesses and parallel execution of the tasks.

\subsection{OpenACC}

OpenACC \cite{OpenACC} is a framework for heterogeneous systems with accelerator devices. It is designed to simplify the programing paradigm for CPU/GPU systems by abstracting the memory management, kernel creation and GPU management. Like OpenMP, it is designed for C, C++ and Fortran, but allowing the parallel task to run on both CPU and GPU at the same time.

While it was originally designed only for CPU/GPU systems, they are currently working on the support for the new Intel Xeon Phi \cite{OpenACC:HPCWire}. Also, they are working alongside with the members of OpenMP to create a new specification supporting accelerator devices in future OpenMP releases \cite{OpenACC:OpenMP}.

\subsection{GAMA}

The GAMA framework \cite{GAMA} is a tool different from OpenACC. It aims to create an abstraction layer between the architectural details of heterogeneous platforms and the programmer, aiding the development of portable and scalable parallel applications. However, unlike OpenACC, its main focus is on obtaining the best performance possible, rather thab abstracting the architecture from the programmer. As consequence, there is more work for the programmer to develop an application, as , for example, it is necessary to define the dicing method (which will be used by GAMA to distribute the dataset). It is also possible to code the kernel that is to run in each device and other specific details.

Even though, the framework frees the programmer from managing the workload distribution, memory usage and data transfers between the available resources. Again, the programmer can have control over these details.

This framework is capble of abstracting the address space of the heterogeneous platform (CPU and the accelerator device memory) used as a shared memory system. One particularity of this tool is that, even though it is capable of handling shared memory systems with one or more multicore CPUs, it is only capable of handling \cuda capable GPUs as accelerator devices on the system.

\subsection{Debugging}

Debugging applications in shared memory systems is a complex task, as the errors are usually harder to replicate than on sequential applications. Bugs can happen due to deadlocks, unexpected changes to the shared memory, data inconsistency and incoherence. While there are some tools to efficiently debug sequential applications, such as the GNU Debugger \cite{GDB}, they lack on the support for multithreaded applications. Unfortunately, there are no debuggers that can efficiently be used to debug a parallel application.

The effort necessary to debug these applications, without the use of any third-party tools, is directly related to the programmers experience and knowledge of working with shared memory systems. However, even the most experienced will face complex obstacles when debugging for more than 4 threads, as the application behavior is much harder to control.

Nvidia offers a tool for debugging \cuda kernels on their GPUs, which is based on the GNU Debugger \cite{NVIDIA:gdb}. It is useful when used to find bugs in the kernels, but only in the same way that a sequential application is debugged. Also, when using more than 2-4 \cuda threads it does not help the programmer at all, considering that \cuda kernels can reach to the thousands of threads.

\newpage
