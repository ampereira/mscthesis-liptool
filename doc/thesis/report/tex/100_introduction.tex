\chapter{Introduction}
\label{Introduction}

\begin{quote}
\textit{The chapter starts to present the dissertation context and the scientific external partners, CERN and LIP and their current research projects. The motivation for the dissertation is presented in section \ref{Motivation}, with the problem contextualized from a physics perspective in subsection \ref{TopQuarkSystem}. The Goals, subsection \ref{Goals}, states the objectives to be achieved by this work, in terms of improving the research and application development quality by implementing a set of solutions for homogeneous and heterogeneous systems, while assessing the efficiency and usability of hardware accelerators in the latter. The scientific contribution of this work is presented in subsection \ref{ScientificContribution}. Subsection \ref{DissertationStructure} overviews the structure of this dissertation.}
\end{quote}

\section{Context}
\label{Context}

Today's computing platforms are becoming increasingly complex with multiple multicore CPU chips per computing node, sometimes coupled with hardware accelerators. While the application performance is an important issue to tackle, the efficient usage of the resources of these systems is a crucial subject that needs to be addressed. Guaranteeing that the available computational resources are being fully used by an application may require deep knowledge of the underlying architecture details of both CPUs and hardware accelerators or extensive tunning. It is important to understand the resources on a CPU, such as the computing units organization, benefits and limitations of using multiple cores, and cache memory hierarchy, to avoid underusing the full computational potential of the device. The architecture design of many-core hardware accelerators have significant differences from device to device with no standard yet defined, unlike current CPUs. The programmer must know the architectural details of each hardware accelerator to produce efficient code. From the hardware point of view, efficiency may also translate in the ratio between power usage and computational performance.

The memory organization varies between homogeneous and heterogeneous systems, where the first use a shared memory paradigm and the latter a distributed memory paradigm. This means that the data is always available for the CPUs on a system but, when considering hardware accelerators that are coupled by a PCI-Express interface, data must be explicitly passed between the CPU and the device. The management of the data may affect the performance and efficiency of an application. Code parallelism is a must to take advantage of the multiple cores in both the CPUs and the hardware accelerators, with the programming model differing from shared to distributed memory environments. Data races, resource contention and, in heterogeneous systems, explicit memory transfers are complex issues that the programmer must tackle. Also, each accelerator manufacturer uses their own frameworks and compilers for programming their devices. With the current computational systems rapidly changing, scientists restrain from investing on academic formation in computer science, opting for self-learning these complex principles. These factors reinforced the collaboration of multidisciplinary teams of scientists from various fields with computer scientists to develop high performing, efficient and robust applications.

The European Organization for Nuclear Research \cite{CERN} (CERN, acronym for \textit{Conseil Européen pour la Recherche Nucléaire}) is a consorcium of 20 european countries, with the purpose of operating the largest particle physics laboratory in the world. Founded in 1954, CERN is located in the border between France and Switzerland, and employs thousands of scientists and engineers representing 608 universities and research groups of 113 different nationalities.

CERN research focus on the basic constituents of matter, which started by studying the atomic nucleus but quickly progressed into high energy physiscs (HEP), namely on the interactions between particles. The instrumentation used in nuclear research is essentially divided into particle accelerators and detectors, alongside with the facilities necessary for delivering the protons to the accelerators. The purpose of the accelerator is to speed up groups of particles close to the speed of light, in opposite directions, resulting in a controlled collision inside the detectors (each collision is considered an event). The detectors record various characteristics of the resultant particles, such as energy and momentum, which originate from complex decay processes of the collided protons. The purpose of these experiments is to test and validate specific HEP theories and models by interpreting the results of the collisions and test if they match the expected theoretical model.

CERN laboratory started with a small low energy particle accelerator, the Proton Synchrotron \cite{CERN:PS} inaugurated in 1959, but soon its equipment was iteratively upgraded and expanded. The current facilities are constituted by the older accelerators (some already decomissioned) and particle detectors, as well as the newer Large Hadron Collider (LHC) \cite{CERN:LHC} high energy particle accelerator, located 100 meter underground and with a 27 km circuference length. There are currently seven experiments running on the LHC: CMS \cite{CERN:CMS}, ATLAS \cite{CERN:ATLAS}, LHCb \cite{CERN:LHCb}, MoEDAL \cite{CERN:MoEDAL}, TOTEM \cite{CERN:TOTEM}, LHC-forward \cite{CERN:LHCf} and ALICE \cite{CERN:ALICE}. Each of these experiments have their own detector on the LHC and conduct HEP experiments, using distinct technologies and research approaches. One of the most relevant researches being conducted at CERN is the validation of the Higgs boson theory. The ATLAS experiment, a key project at CERN, is conducting most of the crucial experiments on both the top quark and Higgs boson theories. During the next year the LHC will be upgraded to increase its luminosity, e.g., the amount of energy of the accelerated particle beams.

Approximately 600 millions of collisions occur every second at the LHC. Particle detectors react with the particles resultant from the collisions, generating massive amounts of raw data as electric signals. It is estimated that all the detectors combined produce 25 petabytes of data per year \cite{CERN:DATA1,CERN:DATA2}. CERN does not have the financial resources to afford the computational power necessary to process all the data, which motivated the creation of the Worldwide LHC Computing Grid \cite{CERN:WLHCCG}, a distributed computing infrastructure that uses the resources of scientific community for data processing. The grid is organized in a hierarchy divided in 4 tiers. Each tier is made by one or more computing centres and has a set of specific tasks and services to perform, such as store, filter, refine and analyse all the data gathered at the LHC.

The Tier-0 is the data centre located at CERN. It provides 20\% of the total grid computing capacity, and its goal is to store and reconstruct the raw data gathered at the detectors in the LHC, converting it into meaningful information, usable by the remaining tiers. The data is received on a format designed for this reconstruction, with information about the event, detector and software diagnostics. The output of the reconstruction has two formats, the Event Summary Data (ESD) and the Analysis Object Data (AOD), each with different purposes, containing information of the reconstructed objects and calibration parameters, which can be used for early analysis. This tier distributes the raw data and the reconstructed output by the 11 Tier-1 computational centres, spread among the different member countries of CERN.

Tier-1 computational centres are responsible for storing a portion of the raw and reconstructed data and provide support to the grid. In this tier, the reconstructed data suffers more reprocessing, refining and filtering the relevante information and reducing the size of the data, now in Derived Physics Data (DPD) format, then transferred to the Tier-2 computational centres. The size of the data for an event is reduced from 3 MB (raw) to 10 kB (DPD). This tier also stores the output of the simulations performed at Tier-2. The Tier-0 centre is connected to the 11 Tier-1 centres by high bandwidth optical fiber links, which form the LHC Optical Private Network.

There are roughly 140 Tier-2 computational centres spread around the world. Their main purpose is to perform both Monte-Carlo simulations and a portion of the events reconstructions, with the data received from the Tier-1 centres. The Tier-3 centres range from university clusters to small personnal computers, and they perform most of the events reconstruction and final data analysis. In the CERN terminology, an analysis is the denomination of an application which is designed to process a given amount of data in order to extract relevant physics information about events that may support a specific HEP theory.

The Laboratório de Instrumentação e Física Experimental de Partículas (LIP) \cite{LIP} is a portuguese scientific and technical association for research on experimental high energy physics and associated instrumentation. LIP has a strong collaboration with CERN as it was the first scientific organization from Portugal that joined CERN, in 1986. It has laboratories in Lisbon, Coimbra and Minho and 170 people employed. LIP researchers have produced several applications for testing various HEP theories of the ATLAS experiment that use Tier-3 computational resources for data analysis. Most of the analysis applications use in-house developed skeleton libraries, such as the LipCbrAnalysis and LipMiniAnalysis.

The motivation for this dissertation, presented in section \ref{Motivation}, results from a close cooperation between the Department of Informatics of the University of Minho (UM) and the LIP laboratory at UM, which began in 2011.

\section{Motivation, goals \& scientific contribution}
\label{Motivation}

With an increase in particle collisions and data being produced by the detectors at the LHC, research groups will need a larger budget to aquire and maintain the required computational resources to keep up with the analysis. Adding to this increase, research groups working on the same experiment enforce positive competition to find and publish relevant results. The amount and quality of event processing has a direct impact on the research, meaning that groups with more efficient computational resources become ahead of the competition.

Better results are not only obtained by increasing the amount of events analysed; it is important to take into account the quality of each event analysis. The ATLAS detector has an experimental resolution of 2\%, meaning that each measured value from a collision might not be exact and, therefore, the analysis must consider an associated error. It is possible to improve the analysis quality by randomly simulating up to 2\% variations on the detector measured values, but it will increase the analysis execution time, creating a trade-off between events to analyse and their quality. This issue will be further detailed in subsection \ref{TopQuarkSystem}.

One of the most important researches conducted by LIP is related to the discovery of new physics in the top quarks and Higgs boson. An application was devised to reconstruct an event obeying the theoretical model of top quark decay. It also attempts to reconstruct the Higgs boson associated with the event. Each event can be reconstructed several times, with a random offset applied to the parameters (with a maximum magnitude of $\pm1\%$ of the original value), so that a more accurate final reconstruction is obtained by chosing the partial reconstruction that better satisfies the theoretical model. The purpose of this mechanism is to overcome the experimental resolution of the ATLAS detector. The number of reconstructions performed per event directly relates to the application execution time. The theoretical model for this system is presented in subsection \ref{TopQuarkSystem} and the analysis application in chapter \ref{Application}.

While investing in the upgrade of the computational resources of the research group is a valid option to deal with the increase of events to analyse, it is also necessary to take into account if the current resources are being efficiently used by the current applications. Hardware is not necessarily getting faster, but wider as the number of cores per chip is increasing rather that its clock frequency (see chapter \ref{TechnologicalBackground}), which can cause large investments to result in small performance improvements. Current computing clusters are based on computing nodes with one or more multicore CPUs (homogeneous systems) and some coupled with hardware accelerators, which can be very fast and efficient for specific problem domains (heterogeneous systems). It is important to have a knowledge of the newer architectures to develop efficient applications that resort to parallelism to efficiently use the system resources. Programming for such architectures (both multicore CPUs and hardware accelerators) requires a set of skills and expertise that most physicists lack (usually self-taught programmers), which leads to developed applications that do not adequatly use the full potential of these systems.

Increasing the efficiency of an application by resorting to parallelism enables the possibility of performing more reconstructions per event and more events to be processed, while using all the potential of the available computational resources and avoiding needless investments in hardware upgrades.

\subsection{The top quark system and Higgs boson decay}
\label{TopQuarkSystem}

In the LHC, two proton beams are accelerated close to the speed of light in opposite directions, set to collide inside a specific particle detector. From this head-on collision results a chain reaction of decaying particles, and most of the final particles react with the detector allowing their characteristics to be recorded. One of the experiments being conducted at the ATLAS detector is related to the discovery of new top quark physics. Figure \ref{fig:TopQuarkDecay} presents the schematic representation of the top quark decay (usually addressed as the \ttbar system), resultant from a head-on collision of two protons.

\begin{figure}[!htp]
	\begin{center}
		\includegraphics[scale=0.6]{../../common/img/ttbar.png}
		\caption{Schematic representation of the \ttbar system.}
		\label{fig:TopQuarkDecay}
	\end{center}
\end{figure}

The ATLAS detector can record the characteristics of the bottom quarks, detected as a jet of particles, and leptons, the muon (with a positive charge) and electron (with a negative charge). However, the neutrinos do not react with the detector and, therefore, their characteristics are not recorded. To reconstruct the top quarks it is necessary to have the information of all the final particles, so the neutrino characteristics need to be determined. The neutrinos characteristics can be analitically calculated with the information of the quarks and leptons, using a set of properties that determine the behaviour of the \ttbar system. The process of reconstructing the neutrinos is known as the kinematical reconstruction. The reconstruction of the whole \ttbar system has a degree of certainty associated, which determines its quality. The quality of these reconstructions directly impact the research outcome.

The amount of detected bottom quark jets and leptons may vary between events, due to other reactions simultaneously occurring to the top quark decay. As represented in figure \ref{fig:TopQuarkDecay}, 2 jets and 2 leptons are needed to reconstruct the \ttbar system, but the input data for an event may have many more of these particles associated. It is necessary to reconstruct the neutrinos, and then the whole system, for every available combination of 2 jets and 2 leptons in the input data (often referred only as \textit{combination}) and only chose the most accurate reconstruction, for each given event.

Another factor affecting the quality of the reconstruction is the experimental resolution of the ATLAS particle detector, which associates an error up to 2\% with every measurement made. If the measurements of the jets and leptons are not accurate enough, the kinematical reconstruction will produce inaccurate neutrinos and affect the overall reconstruction of an event. This might render useless an event that otherwise would provide relevant physics information. It is possible to overcome this problem by performing the whole \ttbar system reconstruction a given amount of times for each combination of 2 bottom quark jets with 2 leptons, randomly varying the particle characteristics (momentum, energy and mass), with a maximum magnitude of 2\% of the original value. The amount of variations performed per combination will directly be proportional the final quality of the event reconstruction, as more of the search scope is covered(defined by the experimental resolution error), compared to performing a single \ttbar system reconstruction.

The search for the Higgs boson is also part of the research being conducted at LIP. Figure \ref{fig:HiggsBosonDecay} shows the Higgs boson and top quarks decay. It is possible to reconstruct the Higgs boson from the two Bottom Quark jets that it decays to, and it can be performed alongside the \ttbar system reconstruction. This adds at least two more jets to the event information, and it is not possible to know before the reconstruction which jets belong to the Higgs decay or the top quarks decay. The Higgs reconstruction must be performed after the \ttbar system reconstruction, in such a way that the jets chosen to reconstruct it must not be the ones used in the \ttbar system reconstruction. Adding this new jets increases the number of jets/leptons combinations to test in the kinematical reconstruction, and the Higgs boson must always be reconstructed for each \ttbar system. The overall quality of the event reconstruction depends on the quality of both \ttbar system and Higgs boson reconstructions.

\begin{figure}[!htp]
	\begin{center}
		\includegraphics[scale=0.6]{../../common/img/ttbar_higgs.png}
		\caption{Schematic representation of the \ttbar system with the Higgs boson decay.}
		\label{fig:HiggsBosonDecay}
	\end{center}
\end{figure}

The analysis presented in this section is performed by the \tth application developed by LIP researchers. The application receives an input data file with a set of events data and tries to reconstruct both the \ttbar system and the Higgs boson of each event, using the described methodology. These files are typically 1 GB long and the LIP process huge quantities using this application for the research on top quarks and Higgs boson physics. A in-depth computational analysis of \tth is presented in chapter \ref{Application}, where its flow is presented, computationally characterized and the critical region affecting the performance is identified.

\subsection{Goals}
\label{Goals}

\tth must to perform more variations per event by improving the performance of the top quark and Higgs boson reconstructions (the critical region), boosting the quality of the results, within the same, or less, time that the current non-varied event processing. 

The main goal of this dissertation is to take the \tth scientific application coded by physicists, where the main concern during its development was the correcteness of the code rather than its performance, and improve its efficiency by (i) identifying the bottlenecks and optimizing the code, (ii) increasing the performance by resorting to parallelism for homogeneous and heterogeneous systems, assessing the performance and usability of hardware accelerators for this type of problem, and (iii) the development of a simple scheduler for managing the workload distribution among various instances of the same sequential or parallel application on homogeneous systems (i.e., an application which needs to process a large set of separate input files), without changeing the application.

This work will give a inside perspective of how scientific applications are being developed by self taught programmers with no background in computer science, and help define a set guidelines for coding efficient applications for complex parallel systems. All the changes that will be made to the \tth application, including the introduction of parallelism, are as modular as possible from the context of this specific application, in such a way that they are portable to other applications, requiring small changes to the code. The scheduler will offer parallelization of the data to process at the application level, requiring no changes to the application source code. The implementations will be structured so that the parallelization mechanisms and the scheduler can be improved to be later included into the LIP libraries to produce a tool that automatically extracts parallelism from these applications.

\subsection{Scientific contribution}
\label{ScientificContribution}

This dissertation work aimed to improve the performance, efficiency and quality of an analysis application. The application is now able to process more variations and events by efficiently using the computational resources on a system. This improves the quality of the research conducted by LIP on the top quarks and Higgs Boson reconstructions, without requiring any hardware investments. An application scheduler was designed to work with any kind of application, extracting parallelism from simultaneous executions of applications, leading to a more efficient resource usage that can improve the performance of all LIP applications.

The gained expertise from parallelizing these complex applications on both homogeneous and heterogeneous systems allowed to provide a set of guidelines for restructuring the LipMiniAnalysis library that, coupled to the application scheduler, can be later transformed into a tool that automatically extracts parallelism from LIP applications.

\section{Dissertation structure}
\label{DissertationStructure}

This dissertation has 5 chapters with their summary presented below:

\begin{description}
	\item[Introduction] \hfill \\
	This chapter, which presented the scientific contextualization the external partners, CERN and LIP and their current research projects. The motivation for the dissertation is presented in section \ref{Motivation}, followed by the dissertation goals, in terms of improving the research and application development quality. The scientific contribution of this work was then presented in the subsection \ref{ScientificContribution}. Section \ref{DissertationStructure} gave an overview of the structure of this dissertation.
	\item[Computing Background] \hfill \\
	This chapter presents the current technological state of the art in terms of hardware and software. Hardware-wise, both homogeneous and heterogeneous system architectures and details are presented in sections \ref{HomogeneousSystems} and \ref{HeterogeneousSystems}, respectively. The context of current hardware accelerators is also included in the latter. Software-wise background is presented in section \ref{Software}. Various frameworks and libraries are presented for homogeneous systems and accelerators in early subsections. Section \ref{HeterogeneousFrameworks} presents the available frameworks for parallelization in heterogeneous systems, while current solutions for profiling and debugging parallel applications are presented in section \ref{ProfilingDebugging}.
	\item[Top Quark and Higgs Boson Reconstructions] \hfill \\
	The \tth application for event reconstruction is presented in this chapter. Its dependencies are presented. The flow of the application is presented in section \ref{Application:Flow}, together with a schematic representation. Its main functions are presented and the schematic flow is compared against a callgraph of the application to help understanding what happens in each of the most important functions. The critical region is identified in section \ref{CriticalRegion} and characterized in subsection \ref{ComputationalCharactrization}. Some initial optimizations to the code are presented in subsection \ref{InitialOptimizations}.
	\item[Parallelization Approaches] \hfill \\
	From different parallelization alternatives are presented in this chapter. For homogeneous systems, a shared memory parallelization is discussed in section \ref{Parallelization:SharedMem}, where the abstract heuristic used is shown, and the implementation and a performance analysis are presented in subsections \ref{SharedMemImplementation} and \ref{SharedMemPerformance}, respectively. For heterogeneous systems using hardware accelerators, two alternatives are presented: using GPU as an accelerator, in section \ref{Parallelization:GPU}, with its implementation and performance discussed and analysed in subsections \ref{GPUImplementation} and \ref{GPUPerformance}; using the \intel Xeon Phi as an accelerator in section \ref{Parallelization:MIC} and its implementation discussed in subsection \ref{MICImplementation}. A software scheduler for managing workload distribution among applications for homogeneous shared memory systems is presented in section \ref{Parallelization:Scheduler}. Its implementation details and performance analysis are shown in subsections \ref{SchedulerImplementation} and \ref{SchedulerPerformance}.
	\item[Conclusions \& Future Work] \hfill \\
	This chapter concludes the dissertation, presenting an overview of the results obtained by the work developed, on both homogeneous and heterogeneous systems. Guidelines for future work, on improving the test case application and providing parallel solutions abstracted from the programmer for future application development, are presented.
\end{description}
