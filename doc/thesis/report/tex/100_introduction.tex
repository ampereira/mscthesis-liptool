\chapter{Introduction}
\label{Introduction}

\begin{quote}
\textit{The dissertation is first presented by contextualizing the scientific background of CERN and LIP organizations, as well as their current research projects, which are closely involved in this work. The motivation for the dissertation is presented in section \ref{Motivation}, with the problem contextualized from a physics perspective in subsection \ref{TopQuarkSystem}. The Goals, subsection \ref{Goals}, states the objectives to be achieved by this work, in terms of improving the research and application development quality by implementing a set of solutions for homogeneous and heterogeneous systems, while assessing the efficiency and usability of hardware accelerators in the latter. The scientific contribution of this work is presented in subsection \ref{ScientificContribution}. Subsection \ref{DissertationStructure} overviews the structure of this dissertation.}
\end{quote}

\section{Context}
\label{Context}

The European Organization for Nuclear Research \cite{CERN} (CERN, acronym for \textit{Conseil Européen pour la Recherche Nucléaire}) is a consorcium of 20 european countries, with the purpose of operating the largest particle physics laboratory in the world. Founded in 1954, CERN is located in the border between France and Switzerland, and employs thousands of scientists and engineers representing 608 universities and research groups of 113 different nationalities.

CERN research focus on the basic constituents of matter, which started by studying the atomic nucleus but quickly progressed into high energy physiscs (HEP), namely on the interactions between particles. The instrumentation used in nuclear research is essentially divided into particle accelerators and detectors, alongside with the facilities necessary for delivering the protons to the accelerators. The purpose of the accelerator is to speed up groups of particles close to the speed of light, in opposite directions, resulting in a controlled collision inside the detectors (the collision is called an event). The detectors record various characteristics of the resultant particles, such as energy and momentum, which originate from complex decay processes of the collided protons. The purpose of these experiments is to test and validate specific HEP theories by interpreting the results of the collisions based on the expected theoretical model.

CERN laboratory started with a small low energy particle accelerator, the Proton Synchrotron \cite{CERN:PS} inaugurated in 1959, but soon its equipment was iteratively upgraded and expanded. The current facilities are constituted by the older accelerators (some already decomissioned) and particle detectors, as well as the newer Large Hadron Collider (LHC) \cite{CERN:LHC} high energy particle accelerator, located 100 meter underground and with a 27 km circuference length. There are currently seven experiments running on the LHC: CMS \cite{CERN:CMS}, ATLAS \cite{CERN:ATLAS}, LHCb \cite{CERN:LHCb}, MoEDAL \cite{CERN:MoEDAL}, TOTEM \cite{CERN:TOTEM}, LHC-forward \cite{CERN:LHCf} and ALICE \cite{CERN:ALICE}. Each of these experiments have their own detector on the LHC and conduct HEP experiments, using of distinct technologies and research approaches. One of the most popular researches being conducted at CERN is the validation of the Higgs boson theory. During the next year the LHC will be upgraded to increase its luminosity (amount of energy of the accelerated particle beams).

Approximately 600 millions of collisions occur every second at the LHC. Particle detectors react with the particles resultant from the collisions, generating massive amounts of raw data as electric signals. It is estimated that all the detectors combined produce 25 petabytes of data per year \cite{CERN:DATA1,CERN:DATA2}. CERN does not have the financial resources to afford the computational power necessary to process all the data, which motivated the creation of the Worldwide LHC Computing Grid \cite{CERN:WLHCCG}, a distributed computing infrastructure that uses the resources of scientific community for data processing. The grid is organized in a hierarchy divided in 4 tiers. Each tier is made by one or more computing centers and has a set of specific tasks and services to perform, such as store, filter, refine and analyse all the data gathered at the LHC.

The Tier-0 is the data center located at CERN. It provides 20\% of the total grid computing capacity, and its objective is to store and reconstruct the raw data gathered at the detectors in the LHC, converting it into meaningful information, usable by the remaining tiers. The data is received on a format designed for this reconstruction, with information about the event, detector and software diagnostics. The output of the reconstruction has two formats, the Event Summary Data (ESD) and Analysis Object Data (AOD), each with different purposes, containing information of the reconstructed objects and calibration parameters, which can be used for early analysis. This tier distributes the raw data and the reconstructed output by the 11 Tier-1 computational centers, spread among the different countries that are members of CERN.

Tier-1 computational centers are responsible for storing a portion of the raw and reconstructed data and provide support to the grid. In this tier, the reconstructed data suffers more reprocessing, refining and filtering the relevante information and reducing the size of the data, now in Derived Physics Data (DPD) format, then transferred to the Tier-2 computational centers. The size of the data for an event is reduced from 3 MB (raw) to 10 kB (DPD). This tier also stores the output of the simulations performed at Tier-2. The Tier-0 center is connected to the 11 Tier-1 centers by high bandwidth optical fiber links, which form the LHC Optical Private Network.

There are roughly 140 Tier-2 computational centers spread around the world. Their main purpose is to perform Monte-Carlo simulations with the data received from the Tier-1 centers, but also perform a portion of the events reconstructions. The Tier-3 centers range from university clusters to small personnal computers, and they perform most of the events reconstruction and final data analysis. In the CERN terminology, an analysis is the denomination of an application which is designed to process a given amount of data in order to extract physically relevant information about events that may support a specific HEP theory.

\section{LIP Research Group}
\label{LIP}

The Laboratório de Instrumentação e Física Experimental de Partículas (LIP) \cite{LIP} is a portuguese scientific and technical association for research on experimental high energy physics and associated instrumentation. LIP has a strong collaboration with CERN as it was the first scientific organization from Portugal that joined, in 1986. It has laboratories in Lisbon, Coimbra and Minho and 170 people employed. LIP researchers have produced several applications for testing various HEP theories of the ATLAS experiment that use Tier-3 computational resources for data analysis. Most of the analysis applications use home-grown skeleton libraries, such as the LipCbrAnalysis and LipMiniAnalysis.

The motivation for this dissertation, presented in section \ref{Motivation}, results from a close cooperation between the Department of Informatics of the University of Minho and the LIP laboratory in Minho, which began in 2011.

\section{Motivation, Goals \& Scientific Contribution}
\label{Motivation}

With an increase of collisions and, consequently, the data being produced by the detectors at the LHC, research groups will need a bigger budget for aquiring and maintaining computational resources to analyze the data and keep up with the deadlines. To add up to the increase in data, research groups working on the same experiment enforce positive competition to be the first to find and publish relevant results. The amount and quality of event processing as a direct impact on the research, meaning that groups with more computational resources ahead of the competition.

Better results are not only obtained by increasing the amount of events analyzed; it is important to take into account the quality of each event analysis. The ATLAS detector has an experimental resolution of 2\%, meaning that each measured value for a characteristic of a particle resultant from a collision might not be exact and, therefore, the analysis will have an error associated. It is possible to improve the analysis quality but it will increase its execution time, creating a trade-off between events to analyze and their quality. This issue will be presented in the context of this dissertation with more detail on subsection \ref{TopQuarkSystem}.

One of the most important analysis being conducted by LIP relates to the Top Quark and the Higgs Boson researches. An application was devised to reconstruct an event obeying the theoretical model of Top Quark decay. It also attempts to reconstruct the Higgs Boson associated with the event. Each event can be reconstructed several times, with some of its parameters varied by a random offset (with a maximum magnitude of 2\% of the original value), so that a accurate final reconstruction is obtained by chosing the partial reconstruction that better satisfies the theoretical model. The purpose of this mechanism is to overcome the experimental resolution of the ATLAS detector. The number of reconstructions performed per event directly relates to the application execution time. The theoretical model for this system is presented in subsection \ref{TopQuarkSystem} and the analysis application in chapter \ref{Application}.

While investing in the upgrade of the computational resources of the research group is a valid option to deal with the increase of events to analyze, it is also necessary to take into account if the current resources are being efficiently used by the current applications. Also, hardware is not necessarily getting faster, but wider as the number of cores per chip is increasing rather that its speed (see chapter \ref{TechnologicalBackground}), which can cause big investments to result in small improvements. Current computing clusters are constituted of systems with one or more multicore CPUs (homogeneous systems) and some even utilizing hardware accelerators, very fast and efficient for specific problem domains (heterogeneous systems). It is important to have a knowledge of the newer architectures in order to develop efficient applications that resort to parallelism to efficiently use the system resources. Programming for such architectures (both multicore CPUs and hardware accelerators) requires a set of skills and experience that most physicists (usually self-taught programmers) do not have, developing applications not adequate to take advantage of these architectures.

Increasing the efficiency of an application by resorting to parallelism enables the possibility of performing more reconstructions per event and more events to be processed, while using all the potential of the available computational resources and avoiding needless investments in hardware upgrades.

\subsection{The Top Quark system and Higgs boson decay}
\label{TopQuarkSystem}

In the LHC, two proton beams are accelerated close to the speed of light in opposite directions, set to collide inside a specific particle detector. From this head-on collision results a chain reaction of decaying particles, and most of the final particles react with the detector for it to record their characteristics. One of the experiments being conducted at the ATLAS detector is related to the discovery of new Top Quark physics. The schematic representation of the Top Quark decay (usually addressed as the \ttbar system), resultant from a head-on collision of two protons, is presented in figure \ref{fig:TopQuarkDecay}.

\begin{figure}[!htp]
	\begin{center}
		\includegraphics[scale=0.5]{../../common/img/ttbar.png}
		\caption{Schematic representation of the \ttbar system.}
		\label{fig:TopQuarkDecay}
	\end{center}
\end{figure}

The ATLAS detector is able to record the characteristics of Bottom Quarks, which are detected as a jet of particles rather than a single particle, and leptons, the muon (that has a positive charge) and electron (with a negative charge). However, the neutrinos do not react with the detector and, therefore, their characteristics are not recorded. To reconstruct the Top Quarks it is necessary to have the information of all the final particles, so the neutrino characteristics need to be determined. Since the \ttbar system obeys a set of properties, and using the information of the quarks and leptons, the neutrinos characteristics can be analitically calculated. The process of reconstructing the neutrinos is referred as kinematical reconstruction. The reconstruction of the whole \ttbar system has a degree of certainty associated, which determines its quality. The quality of these reconstructions directly impact the research conducted by LIP.

The amount of Bottom Quark jets and leptons detected may vary between events, due to other reactions occurring at the same time of the Top Quark decay. As represented in figure \ref{fig:TopQuarkDecay}, 2 jets and 2 leptons are needed to reconstruct the \ttbar system, but the data for an event may have many more of these particles associated. It is necessary to reconstruct the neutrinos, and then the whole system, for every combination of 2 jets and 2 leptons (often referred only as \textit{combination}) available in the input data, and only chose the most accurate reconstruction for a given event.

Another factor affecting the quality of the reconstruction is the experimental resolution of the ATLAS particle detector, which associates an error up to 2\% with every measurement made. If the measurements of the jets and leptons are not precise enough the kinematical reconstruction will produce inaccurate neutrinos and affect the overall reconstruction of an event. This might render an event useless, which otherwise would provide relevant physics information. It is possible to overcome this problem by performing the whole \ttbar system reconstruction a given amount of times for each combination of 2 Bottom Quark jets with 2 leptons, randomly varying the particle characteristics (momentum, energy and mass of the jets and leptons), with a maximum magnitude of 2\% of the original value. The amount of variations performed per combination will directly proportional the final quality of the event reconstruction, as more of the search scope (defined by the experimental resolution error) is covered, compared to performing a single \ttbar system reconstruction.

The look for the Higgs Boson is also part of the research being conducted at LIP. Figure \ref{fig:HiggsBosonDecay} schematizes the Higgs Boson and Top Quark decay. It is possible to reconstruct the Higgs Boson from the two Bottom Quark jets that it decays to, and it can be performed alongside the \ttbar system reconstruction. This adds at least two more jets to the event information, and it is not possible to know before the reconstruction which jets belong to the Higgs decay or the Top Quark decay. Considering this, the Higgs reconstruction must be performed after the \ttbar system reconstruction, in such a way that the jets chosen to reconstruct it must not be the ones used in the \ttbar system reconstruction. Adding this new jets increases the number of jets/leptons combinations to test in the kinematical reconstruction, and for each \ttbar system reconstruction the Higgs must be also reconstructed. Now, the quality of the event reconstruction depends on the quality of both \ttbar system and Higgs Boson reconstructions.

\begin{figure}[!htp]
	\begin{center}
		\includegraphics[scale=0.5]{../../common/img/ttbar_higgs.png}
		\caption{Schematic representation of the \ttbar system with the Higgs Boson decay.}
		\label{fig:HiggsBosonDecay}
	\end{center}
\end{figure}

This specific analysis presented is performed by an application developed by LIP researchers, the \tth. The application receives input data file with a set of event data and attempts to reconstructs both the \ttbar system and the Higgs Boson of each event, using the processes described. These files are usually 1 GB long and the LIP processes huge quantities for each deadline using this application for researching on Top Quark and Higgs Boson physics. A in-depth computational analysis of \tth is presented in chapter \ref{Application}, where its flow is presented, computationally characterized and the critical region affecting the performance is identified.

\subsection{Goals}
\label{Goals}

It is possible for \tth to perform more variations per event by increasing the performance of the Top Quark and Higgs Boson reconstructions, boosting the quality of the results, within the same, or less, time that the current non-varied event processing occurs. The objective of this dissertation is to take the \tth scientific application made by physicists, which the main concern during its development was the correcteness of the code rather than its performance, and improve its efficiency by (i) identifying the bottlenecks and optimizing the code, (ii) increasing the performance by resorting to parallelism for homogeneous and heterogeneous systems, assessing the performance and usability of hardware accelerators for this type of problem, and (iii) the development of a simple scheduler for managing the workload distribution among various instances of the same sequential or parallel application (i.e., an application which needs to process a large set of separate input files) on homogeneous systems, without changing the application source code.

This work will give a inside perspective of how scientific applications are being developed by self taught programmers with little to no background in computer science, and help define a set guidelines for coding efficient, and possibly parallel, applications. All the changes that will be made to the \tth application, including the introduction of parallelism, will be as modular as possible from the context of this specific application, in such a way that they might be portable to other applications, only requiring. The scheduler will offer parallelization of the data to process at the application level, requiring minor to no changes to the application source. The implementations will be structured in such a way that the parallelization mechanisms and the scheduler can be improved and possibly transformed in a tool used by the researchers at LIP.

\subsection{Scientific Contribution}
\label{ScientificContribution}

This dissertation work aims to improve the quality of a specific research field of LIP, by providing a set of tools and know-how to improve the performance of a set scientific applications and expose the problematic of unefficient usage of computational resources. By improving the applications, and consequently the quality of the research, LIP will gain an advantage over other research groups in the look for new Top Quark and Higgs Boson physics. By experiencing the process of optimizating this kind of scientific applications, it is possible to provide physicists with know-how and tools and mechanisms for optimizing and extract parallelism, increasing the performance of future applications. By developing applications that efficiently use the computational resources it is possible to reduce the investment in new hardware, which otherwise would have small pratical returns.

\section{Dissertation Structure}
\label{DissertationStructure}

This dissertation has 5 chapters and their summary is presented below:

\begin{description}
	\item[Introduction] \hfill \\
	The dissertation is first presented by contextualizing the scientific background of CERN and LIP organizations, as well as their current research projects, which are closely involved in this work. The motivation for the dissertation is presented in section \ref{Motivation}, with the problem contextualized from a physics perspective in subsection \ref{TopQuarkSystem}. The Goals, subsection \ref{Goals}, states the objectives to be achieved by this work, in terms of improving the research and application development quality by implementing a set of solutions for homogeneous and heterogeneous systems, while assessing the efficiency and usability of hardware accelerators in the latter. The scientific contribution of this work is presented in subsection \ref{ScientificContribution}. Subsection \ref{DissertationStructure} overviews the structure of this dissertation.
	\item[Technological Background] \hfill \\
	This chapter presents the current technological state of the art in terms hardware and software. Hardware-wise, both homogeneous and heterogeneous system architectures and details are presented in sections \ref{HomogeneousSystems} and \ref{HeterogeneousSystems}, respectively. A contextualization of current hardware accelerators is also made in the latter. Software-wise is presented in section \ref{Software}. Various frameworks and libraries are presented for homogeneous systems and accelerators in sections \ref{pThreads}, \ref{OpenMP}, \ref{MPI} and \ref{CUDA}. Section \ref{HeterogeneousFrameworks} presents the available frameworks for parallelization in heterogeneous systems. Finally, current solutions for profiling and debugging parallel applications is presented in section \ref{ProfilingDebugging}.
	\item[\tth Application] \hfill \\
	The \tth application for event reconstruction is presented in this chapter. Its dependencies are presented. The flow of the application is presented in section \ref{Application:Flow}, accompanied by a schematic representation. Its main functions are presented and the schematic flow is compared against a callgraph of the application to help understanding what happens in each of the most important functions. The critical region is identified in section \ref{CriticalRegion} and characterized in subsection \ref{ComputationalCharactrization}. Some initial optimizations to the code are presented in subsection \ref{InitialOptimizations}.
	\item[Parallelization Approaches] \hfill \\
	For different parallelization alternatives are presented in this chapter. For homogeneous systems, a shared memory parallelization is discussed in section \ref{Parallelization:SharedMem}, where the abstract heuristic used is shown, and the implementation and a performance analysis are presented in subsections \ref{SharedMemImplementation} and \ref{SharedMemPerformance}, respectively. For heterogeneous systems using hardware accelerators, two alternatives are presented: using GPU as an accelerator, in section \ref{Parallelization:GPU}, with its implementation and performance discussed and analyzed in subsections \ref{GPUImplementation} and \ref{GPUPerformance}; using the \intel Xeon Phi as an accelerator in section \ref{Parallelization:MIC} and its implementation discussed in subsection \ref{MICImplementation}. A software scheduler for managing workload distribution among applications for homogeneous shared memory systems is presented in section \ref{Parallelization:Scheduler}. Its implementation details and performance analysis are shown in subsections \ref{SchedulerImplementation} and \ref{SchedulerPerformance}.
	\item[Conclusions \& Future Work] \hfill \\
	This chapter concludes the dissertation, presenting an overview of the results obtained by the work developed, on both homogeneous and heterogeneous systems. Guidelines for future work, on improving the test case application and providing parallel solutions abstracted from the programmer for future application development, are presented.
\end{description}
