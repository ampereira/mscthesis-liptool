\appendix
\pdfbookmark{Test Methodology}{test methodology}
\chapter{Test Methodology}
\label{App:TestMethodology}

The purpose of this appendix is to present and justify the methodology chosen to perform the performance and algortihm characterization related tests.

All performance measurements, of both the original and parallel algorithms, were made on binaries compiled with the same compiler and same flags, presented in section \ref{App:TestEnv}. All tests used the same input, a file containing 5738 events, from which 1729 reach the \ttDilepKinFit and the rest are discarded in the previous cuts, of a electron-muon collision. The problem size is considered to be the number of variations to do to each combination of the jets and leptons within an event, since the number of combinations varies between events but remains constant overall as the same input is used. The number of variations tested were 2^{x}, where x \in \{1, ..., 10\}. \todo{fix this}

For the shared memory implementation was used 1, 2, 4, 8, 16, 32 and 64 threads. The test using 1 thread has the purpose of evaluating the overhead of the creation and access to the data structures. The test using 64 threads is used to check if the software multithreading (managed by the operating system) has benefits, which can expose problems when accessing memory, specially to the memory bank of the other CPU, where the thread becomes stalled waiting for the data. The 8 thread test will only use one CPU, with one thread per core, running the application without the limitations of the NUMA memory accesses and the multithreading. With 16 threads both CPUs will be fully used, meaning that the memory accesses are now NUMA, but still without using hardware multithreading. The 32 threads test will use both CPUs with hardware multithreading.

In the GPU the number of threads used was the number of variations times the number of combinations, so that each thread computes a variation of a combination. This way there is a high number of threads to hide the memory access latency of the GPU.

It is important to adopt a good heuristic for choosing the best measurement since it is not possible to control the operating system and other background task necessary for the system, which can occasionally need CPU time. The mean is very sensitive to extreme values, i.e., the cases when the system may have a spike on the workload from other OS tasks and greatly affect the measurement will have a big impact on the mean, not truly reflecting the actual performance of the application. The median can be affected by a series of values measured while the system was under some load, even if a small subset of great measurements was made. Choosing only the best measurement, with the lower execution time, is not a solid heuristic, since it is more complex to replicate the result.

The heuristic choosen was the \textit{k best}. It chooses the best value within an interval with other \textit{k} values measured. It is almost as good as the best value heuristic for obtaining the best measurement but also offers a solid result capable of being replicated. It was used a 5\% interval, with a \textit{k} of 4, a minimum of 16 measurements and a maximum of 32 (in case that there are less than \textit{k} values within the interval).