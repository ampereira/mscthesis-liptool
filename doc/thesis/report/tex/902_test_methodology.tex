\appendix
\pdfbookmark{Test Methodology}{test methodology}
\chapter{Test Methodology}
\label{App:TestMethodology}

The purpose of this appendix is to present and justify the methodology use to conduct the performance and algortihm characterization related tests.

All tests used the same application input data, a file containing 5738 events, from which 1867 reach the \ttDilepKinFit and the rest are discarded in the previous cuts, of a electron-muon collision. The problem size is considered to be the number of variations to do to each combination of the jets and leptons within an event. The number of variations tested were $2^{x}$, where $x \in \{1, ..., 9\}$.

Various number of threads was used to conduct the tests of the shared memory implementations, depending on the system to run. The test using 1 thread has the purpose of evaluating the overhead of the creation and access to the data structures, as well as other implementation details relative to the parallelization. There is always one test using more threads than available by the hardware and is used to test if the software multithreading (managed by the operating system) has benefits, which can expose problems of memory access, specially on NUMA systems. The number of threads equal to the number of cores in one CPU, making one thread per core, tests the application without the limitations of the NUMA memory accesses and the multithreading. With the number of threads equal to the total number of cores tests the use of both CPUs, but still not using hardware multithreading. The number of threads equal to the available hardware threads tests both CPUs with hardware multithreading active.

In the GPU, the number of threads used was equal to the number of variations times the number of combinations, so that each thread computes a variation of a combination. This provides a high number of threads to hide the latency of memory accesses of the GPU.

%The tests on the \intel Xeon Phi were conducted on its two different operating modes: native and offloading. In the native mode all the application is executed on the device, as it is possible to use the ROOT and LipCbrAnalysis libraries since the device uses x86 code, even the single threaded portion of the code. Only the accesses to read the data from the hard drive pass by the CPU. In the offloading mode the Xeon Phi acts like a regular GPU, where only a portion of the application code is executed on the device.

It is important to adopt a good heuristic for chosing the best measurement since it is not possible to control the operating system and other background tasks, which can occasionally interfere with the measurements. The mean value is very sensitive to extreme measurements, i.e., the cases when the system has a spike on the workload from other OS tasks and it impacts the measurement, not accurately reflecting the actual performance of the application. The median can be affected by a series of values measured while the system was under some load. Chosing only the best measurement, is not a solid heuristic, since it is more hard to replicate the result.

The heuristic chosen was the \textit{k best} methodology. It accepts the best value within a given range of other \textit{k} measurements. It is almost as good as the best value heuristic for obtaining the best measurement but offers a solid result capable of being replicated. A 5\% interval was used with a \textit{k} of 4, for a minimum of 16 measurements and a maximum of 32 (in case that there are less than \textit{k} values within the interval).

The \texttt{gettimeofday} function from the C standard libraries was used to measure the execution time of the application and its functions, providing microsecond precision that is enough considering that the original application always takes more than 3 seconds to execute on the test systems used. CUDA Events were used for measuring the portion of the code executed on the GPU, ensuring that the times were properly recorded and by synchronizing the kernel execution.
