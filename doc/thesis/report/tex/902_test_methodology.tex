%\appendix
\chapter{Test Methodology}
\label{App:TestMethodology}

The purpose of this appendix is to present the methodology used to conduct all tests related to the performance and algortihm characterization.

All tests use the same application input data file containing 5738 events, from which 1867 reach the \ttDilepKinFit and the rest are discarded in the previous cuts, of a proton-proton collision. The problem size is considered to be the number of variations to do to each combination of the jets and leptons within an event. The number of variations tested was $2^{x}$, where $x \in \{1, ..., 9\}$.

Various number of threads were used to conduct the tests of the shared memory implementations, depending on the system to run. The test using 1 thread has the purpose of evaluating the overhead of the creation and access to the data structures, as well as other implementation details relative to the parallelization. There is always one test using more threads than available by the hardware and its purpose is to test if the software multithreading (managed by the operating system) has benefits, which can expose memory access problems. The number of threads equal to the number of cores in one CPU tests the application without the limitations of the NUMA memory accesses and multithreading. With the number of threads equal to the total number of cores tests the use of both CPUs without hardware multithreading. The number of threads equal to the available hardware threads tests both CPUs with hardware multithreading active.

The number of used threads in the GPU is equal to the number of variations times the number of combinations, so that each thread computes a variation of a combination. This provides a high number of threads to hide the latency of memory accesses of the GPU.

%The tests on the \intel Xeon Phi were conducted on its two different operating modes: native and offloading. In the native mode all the application is executed on the device, as it is possible to use the ROOT and LipCbrAnalysis libraries since the device uses x86 code, even the single threaded portion of the code. Only the accesses to read the data from the hard drive pass by the CPU. In the offloading mode the Xeon Phi acts like a regular GPU, where only a portion of the application code is executed on the device.

It is important to adopt a good heuristic for chosing the best measurement since it is not possible to control the operating system and other background tasks, which can occasionally interfere with the measurements. The mean value is very sensitive to extreme measurements, i.e., the cases when the system has a spike on the workload from other OS tasks and it impacts the measurement, not accurately reflecting the actual performance of the application. The median can be affected by a series of values measured while the system in under load. Chosing only the best measurement, is not a solid heuristic, since it is more hard to replicate the result.

The chosen heuristic was the \textit{k best} methodology. It accepts the best value within a given range of other \textit{k} measurements. It is almost as good as the best value heuristic for obtaining the best measurement, but offers a solid result capable of being replicated. Was used a 5\% interval with a \textit{k} of 4, for a minimum of 16 measurements and a maximum of 32 (in case that there are less than \textit{k} values within the interval).

The \texttt{gettimeofday} function from the C standard libraries was used to measure the execution time of the application and its functions, providing microssecond precision. It is precise enough considering that the original application always takes more than 3 seconds to execute on all the used test systems. CUDA Events were used for measuring the portion of the code executed on the GPU, ensuring that the times were properly recorded and by synchronizing the kernel execution.
