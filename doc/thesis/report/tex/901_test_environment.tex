\appendix
\pdfbookmark{Test Environment}{test environment}
\chapter{Test Environment}
\label{App:TestEnv}

This appendix focuses on fully characterizing the hardware and software used in all performance measurements of the application for the different implementations developed.

For the shared memory implementation testing was used four dual-socket multicore systems with CPUs of different architectures on the SeARCH cluster, with the purpose of testing a wide range of systems commonly used in small physics research group clusters. The first, named compute-711 node, has two \intel Xeon E5-2670 (Sandy Bridge architecture) \ref{Intel:E52650}, using the Quick Path Interconnect (QPI) interface between CPUs, in a Non Unified Memory Access model (NUMA), meaning that the latency of a CPU acessing its own memory bank is lower than accessing the other CPU memory bank. The QPI interface can perform up to 8 GT/s (giga transfers per second) of 2 bytes packets, in each of the two unidirectional links, with a total bandwidth of 32 GB/s. The system features 64 GB of DDR3 RAM with a speed of 1333 MHz, for a maximum bandwidth of 52.7 GB/s. All RAM bandwidths were measured using the STREAM benchmark \ref{STREAM}.

The second system, compute-601 node, has the same amount of RAM at the same speed, with a maximum bandwidth of 28.6 GB/s. The two CPUs are \intel Xeon X5650 (Nehalem architecture). The difference of memory bandwidth is due to the different memory controllers, while the one in Nehalem has 3 memory channels the one in Sandy Bridge has 4. The two CPUs are interconnect by a QPI interface, but with a different speed than the Sandy Bridge, performing 6 GT/s in each of the two unidirectional channels, for a total bandwidth of 24 GB/s.

The third is also an Intel system, compute-401 node, have 8 GB RAM with a maximum bandwidth of 18.4 GB/s. The CPUs are \intel Xeon E5520 (Nehalem architecture). The two CPUs are interconnected by a QPI interface with the same bandwidth as the previous system, 24 GB/s.

The fourth system features two \amd Opteron 6174, being the system with more physical cores. It has 64 GB of DDR3 RAM at 1333 MHz, with a maximum measured bandwidth of 39.8 GB/s. \amd uses HyperTransport (HT) 3.0 technology, a point-to-point interconnection similar to QPI capable of transmitting 4 byte packets through two links, for an aggregate bandwidth of 51.2 GB/s. The characteristics of the CPUs on the three systems are presented in table \ref{tab:CPUS}.

\begin{table}[!htp]
	\begin{center}
		\begin{tabular}{|c|c|c|c|}
			\hline
			\textbf{CPU} & \intel Xeon E5-2670 & \intel Xeon X5650 & \intel Xeon E5520 & \amd Opteron 6174 \\ \hline
			\textbf{Architecture} & Sandy Bridge & Nehalem & Nehalem & Magny\-Cours \\ \hline
			\textbf{Clock Freq.} & 2.60 GHz & 2.66 GHz & 2.3 GHz & 2.2 GHz \\ \hline
			\textbf{\# of Cores} & 8 & 6 & 4 & 12 \\ \hline
			\textbf{\# of Threads} & 16 & 12 & 8 & 12 \\ \hline
			\textbf{L1 Cache} & \specialcell{32 KB I. +\\32 KB D. per Core} & \specialcell{32 KB I. +\\32 KB D. per Core} & \specialcell{32 KB I. +\\32 KB D. per Core} & \specialcell{64 KB I. +\\64 KB D. per Core} \\ \hline
			\textbf{L2 Cache} & 256 KB per Core & 256 KB per Core & 256 KB per Core & 512 KB per Core \\ \hline
			\textbf{L3 Cache} & 20 MB shared & 12 MB shared & 8 MB shared & \- \\ \hline
			\textbf{CPU Interconnection} & QPI @4.0 GHz & QPI @3.2 GHz & QPI @3.2 GHz & HT @3.2 GHz \\ \hline
			\textbf{ISE} & AVX & SSE 4.2 & SSE 4.2 & SSE 4a \\
			\hline
		\end{tabular}
		\caption{Characterization of the CPUs featured in the three test systems.}
		\label{tab:CPUS}
	\end{center}
\end{table}

The compiler used was the GNU compiler version 4.8, using the -O3 optimizations and the AVX/SSE 4.2/SSE 4a (depending on the CPU architecture) instruction set on the code regions that the compiler sees fit. The compiler used for the \intel Xeon Phi implementation was the \intel Compiler (ICC) version 13.1. Both compilers feature the OpenMP version 3.2 used in the shared memory implementation. For the GPU implementation was used the CUDA 5 SDK, in conjunction with the GNU compiler version 4.6.3 for the code to run on the CPU (any later versions are not supported by the \nvidia NVCC compiler). The ROOT \ref{CERN:ROOT} version used was the 5.34/05. Was used the Performance API version 5.0 for measuring the hardware counters of the different CPUs for the characterization of \ttDilepKinFit.
